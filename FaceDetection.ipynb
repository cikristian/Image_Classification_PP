{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbcfe458-0aad-4ee6-acf5-36d921ed6e69",
   "metadata": {},
   "source": [
    "<div style=\"border: 4px groove dodgerblue; border-radius: 5px; margin-bottom: 20px;\">\n",
    "<h1 style=\"text-align: center; padding: 10px 0 15px 0;\">\n",
    "    <span style=\"color:dodgerblue;\">Mask Detection</span></h1>\n",
    "</div>\n",
    "<img src=\"face.jpg\" alt=\"Image\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac4942c-6da1-4e49-8e0f-31bfd3431e30",
   "metadata": {},
   "source": [
    "Author: Ruth Yankson\n",
    "\n",
    "<a id=\"content\"></a>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "<a href=#one>1. Importing Libraries</a>\n",
    "\n",
    "<a href=#two>2. Loading and Preprocessing Data</a>\n",
    "\n",
    "<a href=#three>3. Model Building</a>\n",
    "\n",
    "<a href=#four>4. Model Training</a>\n",
    "\n",
    "<a href=#five>5. Model Evaluation</a>\n",
    "\n",
    "<a href=#six>6. Model Saving</a>\n",
    "\n",
    "<a href=#seven>7. Making Predictions</a>\n",
    "\n",
    "<a href=#eight> 8. Model Summary</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9542faf-50aa-483e-ba9f-50eb11852126",
   "metadata": {},
   "source": [
    "<a id=\"one\"></a>\n",
    "\n",
    "## 1. Importing Libraries\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ All the libraries used for the entire work.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c128a5bd-1785-47f3-afb9-93de5a9dfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a76d3-7e7d-492a-adec-c309a7783907",
   "metadata": {},
   "source": [
    "<a id=\"two\"></a>\n",
    "\n",
    "## 2. Loading and Preprocessing Data\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Loading the train and validation directories containing the images and preprocessing each.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c186e65-9582-4085-8594-1c2034bf77c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directories\n",
    "train_dir = 'Project_Images_1/data/train'\n",
    "validation_dir = 'Project_Images_1/data/validation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbf3a96-f8e6-4cf8-9f0e-94b6a76cce6a",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "The **ImageDataGenerator** configuration enables various transformations to increase the diversity of the training dataset without the need to collect more data. By augmenting the dataset, the model can learn to recognize objects under different conditions, which enhances its ability to generalize to unseen data and can significantly improve its performance in real-world applications.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5c7e011-1ab4-4da6-bffa-8a57453c9ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data augmentation and rescaling for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83fb354f-21b6-43b0-959b-0cbf8ab4a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only rescale for validation (no augmentation)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343177b2-ebdd-406e-a6cf-f905612fa005",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "The **flow_from_directory method** is an efficient way to manage image data for training deep learning models, especially when working with large datasets. It simplifies loading, preprocessing, and augmenting the images in real time during the training process. \n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232a57e7-4d12-4675-b0c4-2ebbde2f9ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6690 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9db87072-ea7e-4fe1-8750-301e16abadb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 864 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=20,\n",
    "    class_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482b35b-4acb-4cf0-b70b-5a58cf7055e1",
   "metadata": {},
   "source": [
    "<a id=\"three\"></a>\n",
    "\n",
    "## 3. Model Building\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ VGG16, a pre-trained model which works through transfer learning, is used to build a Convolutional Neural Network (CNN) model.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45e747-dc21-4f8b-bf7a-bf310fdcb60d",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "Loading the **VGG16** model with these parameters sets the stage for building a custom image classification model using transfer learning. This approach is efficient and effective, especially when working with limited datasets, as it allows the model to leverage previously learned features and adapt them to new tasks.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8af01d8-f5bb-467a-abe4-2c25ee1c58c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the VGG16 model, excluding the top layers\n",
    "conv_base = VGG16(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(224, 224, 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e30cca76-c801-47a1-93f2-d14b781df8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the convolutional base\n",
    "conv_base.trainable = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec538626-cab3-4f65-912f-f1745bc1db34",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "**Models**, from tensorflow.keras, effectively combines the pre-trained VGG16 convolutional base with additional layers to adapt it for the specific task of mask detection. By flattening the output and adding dense layers, dropout for regularization, and a sigmoid output for binary classification, you create a robust model capable of distinguishing between individuals wearing masks and those who are not.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86153c33-524d-49d1-8ec8-047c76a01d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "mask_detection_model = models.Sequential([\n",
    "    conv_base,\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b09d393c-0097-4487-870d-72a4796e2607",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compile the model\n",
    "mask_detection_model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80946033-a18f-43ad-8b07-89fc960eb135",
   "metadata": {},
   "source": [
    "<a id=\"four\"></a>\n",
    "\n",
    "## 4. Model Training\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ The model is trained using generators.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a04112-0bf8-4942-b8af-7ed2a5946157",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "Using the fit method with the specified parameters enables the effective training of ther model on the training dataset while evaluating its performance on the validation dataset. This approach helps ensure that the model learns to generalize well to new, unseen data, a key aspect of building robust machine learning models.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b34c080e-bcf1-4c6a-9bdc-aed4b008fee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruthy\\.conda\\envs\\PythonReview\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 20/100\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:43\u001b[0m 5s/step - accuracy: 0.4717 - loss: 4.0934"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruthy\\.conda\\envs\\PythonReview\\Lib\\site-packages\\PIL\\Image.py:970: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m701s\u001b[0m 7s/step - accuracy: 0.6394 - loss: 1.9555 - val_accuracy: 0.9236 - val_loss: 0.2087\n",
      "Epoch 2/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ruthy\\.conda\\envs\\PythonReview\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 7s/step - accuracy: 0.8776 - loss: 0.2957 - val_accuracy: 0.9815 - val_loss: 0.1087\n",
      "Epoch 3/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 7s/step - accuracy: 0.9023 - loss: 0.2469 - val_accuracy: 0.9711 - val_loss: 0.0770\n",
      "Epoch 4/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 4s/step - accuracy: 0.9118 - loss: 0.2272 - val_accuracy: 0.9653 - val_loss: 0.0930\n",
      "Epoch 5/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 7s/step - accuracy: 0.9063 - loss: 0.2325 - val_accuracy: 0.9769 - val_loss: 0.0685\n",
      "Epoch 6/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m693s\u001b[0m 7s/step - accuracy: 0.9198 - loss: 0.2107 - val_accuracy: 0.9861 - val_loss: 0.0653\n",
      "Epoch 7/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 7s/step - accuracy: 0.9194 - loss: 0.2034 - val_accuracy: 0.9861 - val_loss: 0.0494\n",
      "Epoch 8/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 4s/step - accuracy: 0.9045 - loss: 0.2374 - val_accuracy: 0.9641 - val_loss: 0.0907\n",
      "Epoch 9/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 7s/step - accuracy: 0.9157 - loss: 0.2153 - val_accuracy: 0.9826 - val_loss: 0.0533\n",
      "Epoch 10/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 7s/step - accuracy: 0.9051 - loss: 0.2184 - val_accuracy: 0.9780 - val_loss: 0.0517\n",
      "Epoch 11/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 7s/step - accuracy: 0.9187 - loss: 0.2163 - val_accuracy: 0.9803 - val_loss: 0.0542\n",
      "Epoch 12/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 4s/step - accuracy: 0.9033 - loss: 0.2269 - val_accuracy: 0.9815 - val_loss: 0.0594\n",
      "Epoch 13/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 7s/step - accuracy: 0.9240 - loss: 0.1823 - val_accuracy: 0.9850 - val_loss: 0.0411\n",
      "Epoch 14/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 7s/step - accuracy: 0.9314 - loss: 0.1853 - val_accuracy: 0.9838 - val_loss: 0.0629\n",
      "Epoch 15/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m696s\u001b[0m 7s/step - accuracy: 0.9166 - loss: 0.1937 - val_accuracy: 0.9826 - val_loss: 0.0449\n",
      "Epoch 16/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m371s\u001b[0m 4s/step - accuracy: 0.9082 - loss: 0.1966 - val_accuracy: 0.9792 - val_loss: 0.0573\n",
      "Epoch 17/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m692s\u001b[0m 7s/step - accuracy: 0.9289 - loss: 0.1792 - val_accuracy: 0.9699 - val_loss: 0.0769\n",
      "Epoch 18/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 7s/step - accuracy: 0.9300 - loss: 0.1725 - val_accuracy: 0.9884 - val_loss: 0.0389\n",
      "Epoch 19/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m684s\u001b[0m 7s/step - accuracy: 0.9369 - loss: 0.1680 - val_accuracy: 0.9850 - val_loss: 0.0425\n",
      "Epoch 20/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m370s\u001b[0m 4s/step - accuracy: 0.9510 - loss: 0.1512 - val_accuracy: 0.9850 - val_loss: 0.0376\n",
      "Epoch 21/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 7s/step - accuracy: 0.9295 - loss: 0.1812 - val_accuracy: 0.9815 - val_loss: 0.0460\n",
      "Epoch 22/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m702s\u001b[0m 7s/step - accuracy: 0.9244 - loss: 0.1994 - val_accuracy: 0.9861 - val_loss: 0.0378\n",
      "Epoch 23/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m687s\u001b[0m 7s/step - accuracy: 0.9392 - loss: 0.1509 - val_accuracy: 0.9815 - val_loss: 0.0538\n",
      "Epoch 24/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m372s\u001b[0m 4s/step - accuracy: 0.9230 - loss: 0.1901 - val_accuracy: 0.9850 - val_loss: 0.0463\n",
      "Epoch 25/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m691s\u001b[0m 7s/step - accuracy: 0.9408 - loss: 0.1505 - val_accuracy: 0.9838 - val_loss: 0.0451\n",
      "Epoch 26/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m689s\u001b[0m 7s/step - accuracy: 0.9222 - loss: 0.1676 - val_accuracy: 0.9815 - val_loss: 0.0564\n",
      "Epoch 27/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m700s\u001b[0m 7s/step - accuracy: 0.9264 - loss: 0.1693 - val_accuracy: 0.9826 - val_loss: 0.0462\n",
      "Epoch 28/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 4s/step - accuracy: 0.9494 - loss: 0.1646 - val_accuracy: 0.9850 - val_loss: 0.0412\n",
      "Epoch 29/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m694s\u001b[0m 7s/step - accuracy: 0.9241 - loss: 0.1774 - val_accuracy: 0.9815 - val_loss: 0.0536\n",
      "Epoch 30/30\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m690s\u001b[0m 7s/step - accuracy: 0.9285 - loss: 0.1726 - val_accuracy: 0.9861 - val_loss: 0.0416\n"
     ]
    }
   ],
   "source": [
    "history = mask_detection_model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=100,\n",
    "    epochs=30,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e726c1f-796d-448f-b091-5dc662229e41",
   "metadata": {},
   "source": [
    "<a id=\"five\"></a>\n",
    "\n",
    "## 5. Model Evaluation\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Evaluating the model on the validation set.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c5df1df-23b6-4a75-8fba-6048d3437755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m44/44\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 5s/step - accuracy: 0.9914 - loss: 0.0306\n",
      "Validation Accuracy: 98.61%\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = mask_detection_model.evaluate(validation_generator)\n",
    "print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabcf3f-3127-44b9-96e1-1767abe2624e",
   "metadata": {},
   "source": [
    "<a id=\"six\"></a>\n",
    "\n",
    "## 6. Model Saving\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Saving the model towards reusability.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "429ed577-5cb2-4be1-8aa8-5068c6391506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "mask_detection_model.save('mask_detection_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00677730-03df-4bc3-b62d-cabbadefe2a4",
   "metadata": {},
   "source": [
    "<a id=\"seven\"></a>\n",
    "\n",
    "## 7. Making Predictions\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Making predictions with neutral images.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "225f64dd-5a44-46cf-9d59-48976f2a0bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask_status(model, img_path):\n",
    "    \"\"\"\n",
    "    Predicts whether a person is wearing a mask or not.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained mask detection model.\n",
    "    - img_path: The file path to the image to be evaluated.\n",
    "\n",
    "    Returns:\n",
    "    - str: A message indicating whether a mask is detected or not.\n",
    "    \"\"\"\n",
    "    # Load the image\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    \n",
    "    # Convert the image to an array and preprocess\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0) / 255.0  # Normalize the image\n",
    "\n",
    "    # Make a prediction\n",
    "    prediction = model.predict(img_array)\n",
    "\n",
    "    # Interpret the prediction\n",
    "    if prediction[0] > 0.5:\n",
    "        return \"No Mask Detected\"\n",
    "    else:\n",
    "        return \"Mask Detected\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbe4a009-256f-4efc-afae-5904f43a436c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Mask Detected'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_mask_status(mask_detection_model, 'masktest.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f049c3af-73fe-466d-8002-b9037d630c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No Mask Detected'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_mask_status(mask_detection_model, 'masktest2.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbb880a-97e0-407c-9672-e6ceb4eb920d",
   "metadata": {},
   "source": [
    "<a id=\"eight\"></a>\n",
    "\n",
    "## 8. Model Summary\n",
    "\n",
    "<a href=\"#content\">\n",
    "    Back to Table of Contents\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n",
    "⚡ Summary of the model.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048f8da7-4390-4683-b845-4dcb5a57b099",
   "metadata": {},
   "source": [
    "<div style=\"padding: 15px; border: 1px solid dodgerblue; border-radius: 5px; background-color: aliceblue\">\n",
    "\n",
    "<div style=\"padding: 15px; border-radius: 5px; background-color: white\">\n",
    "\n",
    "### Summary of the Mask Detection Model\r\n",
    "\r\n",
    "The mask detection model is a deep learning-based solution designed to accurately identify whether individuals are wearing masks or not in images. Built upon the VGG16 architecture, the model leverages transfer learning to effectively adapt pre-trained weights from the ImageNet dataset for the specific task of mask detection. Below are the key aspects of the model:\r\n",
    "\r\n",
    "1. **Architecture**:\r\n",
    "   - The model uses the convolutional base of VGG16, a well-known convolutional neural network (CNN) architecture that excels in image classification tasks.\r\n",
    "   - The convolutional layers of VGG16 extract relevant features from the input images, while additional layers (fully connected and dropout) are added to enable the model to classify the images into two categories: \"mask\" and \"no mask.\"\r\n",
    "\r\n",
    "2. **Data Preprocessing**:\r\n",
    "   - The images are preprocessed using normalization (scaling pixel values to the range [0, 1]) and data augmentation techniques (such as rotation, width and height shifts, zoom, and horizontal flipping) to enhance the model's ability to generalize across various scenarios.\r\n",
    "\r\n",
    "3. **Training Process**:\r\n",
    "   - The model is trained using a binary cross-entropy loss function and the Adam optimizer, which efficiently updates the weights during training.\r\n",
    "   - The training process involves using generators to yield batches of images and their corresponding labels, allowing for effective handling of large datasets.\r\n",
    "\r\n",
    "4. **Performance**:\r\n",
    "   - The model achieved a high validation accuracy of 98.38%, demonstrating its ability to accurately classify images of individuals with and without masks.\r\n",
    "   - This level of accuracy indicates a strong generalization capability, making the model reliable for real-world applications.\r\n",
    "\r\n",
    "5. **Usability**:\r\n",
    "   - The model can be easily integrated into applications requiring mask detection, such as security systems, health monitoring, and compliance with safety regulations.\r\n",
    "   - A reusable function has been implemented to facilitate image predictions, allowing users to quickly assess whether a person in a given image is wearing a mask.\r\n",
    "\r\n",
    "6. **Future Enhancements**:\r\n",
    "   - Potential enhancements include fine-tuning the model by unfreezing some layers of the convolutional base for further training on more extensive datasets.\r\n",
    "   - Additional work can be done to improve model robustness against edge cases, such as varying angles, occlusions, and diverse lighting conditions.\r\n",
    "\r\n",
    "### Conclusion\r\n",
    "\r\n",
    "The mask detection model is a powerful tool for identifying mask usage in images, leveraging advanced deep learning techniques and a well-established architecture. Its high accuracy and ease of use make it suitable for deployment in various scenarios, especially in contexts where public health and safety are a priority. Future improvements could further enhance its performance and applicability in diverse environments.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5af829-bf80-4a7c-9cbc-be753ad12144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
